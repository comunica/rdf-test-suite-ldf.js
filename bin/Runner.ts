#!/usr/bin/env node
import {existsSync, mkdirSync, writeFileSync} from "fs";
import * as minimist from 'minimist';
import {StreamWriter} from "n3";
import * as Path from "path";
import {ITestResult, Util} from "rdf-test-suite";
import { LdfTestSuiteRunner, ILdfTestSuiteConfig } from "../lib/LdfTestSuiteRunner";
import { logger } from "../lib/factory/Logger";

const args = minimist(process.argv.slice(2));

if (args._.length < 2) {
  console.error(`${Util.withColor(`rdf-test-suite-ldf executes test suites for engine-testing. 
rdf-test-suite-ldf currently supports testing for the following sourcetypes: HDT, LD-Files, RDFJ, SPARQL, TPF.`, Util.COLOR_CYAN)}

${Util.withColor('Usage:', Util.COLOR_YELLOW)}
  rdf-test-suite-ldf path/to/myengine.js http://w3c.github.io/rdf-tests/sparql11/data-sparql11/manifest-all.ttl
  rdf-test-suite-ldf path/to/myengine.js http://w3c.github.io/rdf-tests/sparql11/data-sparql11/manifest-all.ttl \
-s http://www.w3.org/TR/sparql11-query/
  rdf-test-suite-ldf path/to/myengine.js http://w3c.github.io/rdf-tests/sparql11/data-sparql11/manifest-all.ttl \
-o earl -p earl-meta.json > earl.ttl

${Util.withColor('Options:', Util.COLOR_YELLOW)}
  -o    output format (detailed, summary, eurl, ... defaults to detailed)
  -p    file with earl properties, autogenerated from package.json if not available (only needed for EARL reports)
  -s    a specification URI to filter by (e.g. http://www.w3.org/TR/sparql11-query/)
  -c    enable HTTP caching at the given directory (disabled by default)
  -e    always exit with status code 0 on test errors
  -t    regex for test IRIs to run
  -i    JSON string with custom options that need to be passed to the engine
  -d    time out duration for test cases (in milliseconds, default 30000)
  -m    URL to local path mapping (e.g. 'https://w3c.github.io/json-ld-api/~/path/to/folder/')
  -r    The port number on which the mocking servers will start spawning (10000 by default)
  -v    Time (ms) to wait before stopping the server after each completed test
`);
  process.exit(1);
}

// Enable caching if needed
let cachePath: string = null;
if (args.c) {
  if(args.c !== true && ! args.c.endsWith('/')){
    console.error(Util.withColor(`Please give a correct caching path. The path '${args.c}' is invalid. Did you forget a trailing slash?`, Util.COLOR_YELLOW));
    process.exit(1);
  }
  cachePath = Path.join(process.cwd(), (args.c === true ? '.rdf-test-suite-cache/' : args.c));
  logger.info(`Caching enabled in ${cachePath}`);
  if (!existsSync(cachePath)) {
    mkdirSync(cachePath);
  }
}

// Import the engine
const engine = require(Path.join(process.cwd(), args._[0]));

const defaultConfig = {
  exitWithStatusCode0: false,
  outputFormat: 'detailed',
  timeOutDuration: 30000,
  // A higher timeOutDuration than the original rdf-test-suite because of more overhead w/ mock servers
};

const config: ILdfTestSuiteConfig = {
  cachePath,
  customEngingeOptions: args.i ? JSON.parse(args.i) : {},
  exitWithStatusCode0: !!args.e || defaultConfig.exitWithStatusCode0,
  outputFormat: args.o || defaultConfig.outputFormat,
  specification: args.s,
  testRegex: new RegExp(args.t),
  timeOutDuration: args.d || defaultConfig.timeOutDuration,
  urlToFileMapping: args.m,
  startPort: args.r,
  serverTerminationDelay: parseInt(args.v, 10) || 10,
};

// Fetch the manifest, run the tests, and print them
const ldfTestSuiteRunner = new LdfTestSuiteRunner();
ldfTestSuiteRunner.runManifest(args._[1], engine, config)
  .then((testResults) => {
    switch (config.outputFormat) {
    case 'earl':
      if (!args.p) {
        throw new Error(`EARL reporting requires the -p argument to point to an earl-meta.json file.`);
      }
      // Create properties file if it does not exist
      if (!existsSync(Path.join(process.cwd(), args.p))) {
        writeFileSync(Path.join(process.cwd(), args.p),
          JSON.stringify(ldfTestSuiteRunner.packageJsonToEarlProperties(require(Path.join(process.cwd(), 'package.json'))),
            null, '  '));
      }
      (<any> ldfTestSuiteRunner.resultsToEarl(testResults, require(Path.join(process.cwd(), args.p)), new Date()))
        .pipe(new StreamWriter({ format: 'text/turtle', prefixes: require('../lib/prefixes.json') }))
        .pipe(process.stdout)
        .on('end', () => onEnd(testResults));
      break;
    case 'summary':
      ldfTestSuiteRunner.resultsToText(process.stdout, testResults, true);
      onEnd(testResults);
      break;
    default:
      ldfTestSuiteRunner.resultsToText(process.stdout, testResults, false);
      onEnd(testResults);
      break;
    }
  }).catch(console.error);

function onEnd(testResults: ITestResult[]) {
  // Exit with status code 1 if there was at least one failing test
  if (!config.exitWithStatusCode0) {
    for (const testResult of testResults) {
      if (!testResult.skipped && !testResult.ok) {
        process.exit(1);
      }
    }
  }
}
